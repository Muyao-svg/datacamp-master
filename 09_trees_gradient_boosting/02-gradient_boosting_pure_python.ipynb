{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opening the box of gradient boosting decision trees (GBDT)\n",
    "\n",
    "```\n",
    "Authors: Alexandre Gramfort\n",
    "         Thomas Moreau\n",
    "```\n",
    "\n",
    "The aim of this notebook is 2-folds:\n",
    "\n",
    "- dissecting a pure Python (unoptimized) implementation of GBRT\n",
    "- use a profiler (here snakeviz) to identify the computational bottleneck.\n",
    "\n",
    "We will explore the code in the file `tinygbt.py`.\n",
    "\n",
    "TinyGBT (Tiny Gradient Boosted Trees) is a 200 line gradient boosted trees implementation written in pure Python.\n",
    "\n",
    "First let's load some regression data.  \n",
    "Gradient Boosting Decision Trees（GBDT，梯度提升决策树）是一类把很多“弱决策树”按顺序叠加起来、通过梯度下降思想不断修正错误的强模型。\n",
    "它在结构上是树模型，在思想上是优化问题。单棵树预测能力有限，但很多棵加起来很强。  \n",
    "\n",
    "我们分类的目的是:minimise loss\n",
    "每一个样本都有一个weight，$(x_i, y_i,w_i)$. 每次训练新的classifier的时候，改变weight-->  \n",
    "        错误分类的，基于更大的weight，正确分类的给予更小的weight.每一个literation都进行这个操作，直到正确分类。  \n",
    "这样实现weak decision --> fort decision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print('Loading data...')\n",
    "# load or create your dataset\n",
    "df_train = pd.read_csv('datasets/regression.train', header=None, sep='\\t')\n",
    "df_test = pd.read_csv('datasets/regression.test', header=None, sep='\\t')\n",
    "\n",
    "df = pd.concat([df_train, df_test], axis=0)\n",
    "y = df[0].values\n",
    "X = df.drop(0, axis=1).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will split the data in train and test. Here the test data will be\n",
    "used as **validation set** to do **\"early stopping\"** i.e. to stop adding trees\n",
    "to the model if the validation loss increases for multiple successive boosting rounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's run our TinyGBT code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "Training until validation scores don't improve for 5 rounds.\n",
      "Iter   0, Train's L2: 0.3586431862, Valid's L2: 0.3570793975, Elapsed: 5.87 secs\n",
      "Iter   1, Train's L2: 0.1879371547, Valid's L2: 0.2210272602, Elapsed: 5.94 secs\n",
      "Iter   2, Train's L2: 0.1789110700, Valid's L2: 0.2200622122, Elapsed: 5.94 secs\n",
      "Iter   3, Train's L2: 0.1763577214, Valid's L2: 0.2190739491, Elapsed: 6.00 secs\n",
      "Iter   4, Train's L2: 0.1759371507, Valid's L2: 0.2190762446, Elapsed: 5.99 secs\n",
      "Iter   5, Train's L2: 0.1757874497, Valid's L2: 0.2190423557, Elapsed: 6.36 secs\n",
      "Iter   6, Train's L2: 0.1757632946, Valid's L2: 0.2190426775, Elapsed: 5.96 secs\n",
      "Iter   7, Train's L2: 0.1757561206, Valid's L2: 0.2190428308, Elapsed: 6.03 secs\n",
      "Iter   8, Train's L2: 0.1757539749, Valid's L2: 0.2190428819, Elapsed: 6.17 secs\n",
      "Iter   9, Train's L2: 0.1757533318, Valid's L2: 0.2190428976, Elapsed: 8.07 secs\n",
      "Training finished. Elapsed: 62.33 secs\n",
      "Start predicting...\n",
      "The MSE of prediction is: 0.21904235565991395\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from tinygbt import GBT\n",
    "\n",
    "print('Start training...')\n",
    "gbt = GBT(n_estimators=10, gamma=0., lambd=1,\n",
    "          min_split_gain=0.1, max_depth=5,\n",
    "          learning_rate=0.3)\n",
    "gbt.fit(X_train, y_train, valid_set=(X_test, y_test), early_stopping_rounds=5)\n",
    "\n",
    "print('Start predicting...')\n",
    "y_pred = gbt.predict(X_test)\n",
    "print('The MSE of prediction is:', mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbm\n",
      "  Downloading lightgbm-4.6.0-py3-none-win_amd64.whl.metadata (17 kB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\lenovo\\anaconda3\\envs\\jupyter\\lib\\site-packages (from lightgbm) (2.3.5)\n",
      "Requirement already satisfied: scipy in c:\\users\\lenovo\\anaconda3\\envs\\jupyter\\lib\\site-packages (from lightgbm) (1.16.3)\n",
      "Downloading lightgbm-4.6.0-py3-none-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   ---------------------------- ----------- 1.0/1.5 MB 4.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 4.5 MB/s  0:00:00\n",
      "Installing collected packages: lightgbm\n",
      "Successfully installed lightgbm-4.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install lightgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first make sure this code gives comparable results with [lightgbm](https://lightgbm.readthedocs.io/en/latest/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "train() got an unexpected keyword argument 'early_stopping_rounds'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mStarting training...\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# train\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m gbm = \u001b[43mlgb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m                \u001b[49m\u001b[43mlgb_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m                \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m                \u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlgb_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m                \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mSaving model...\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# save model to file\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: train() got an unexpected keyword argument 'early_stopping_rounds'"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "# create dataset for lightgbm\n",
    "lgb_train = lgb.Dataset(X_train, y_train)\n",
    "lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n",
    "\n",
    "# specify your configurations as a dict\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': {'l2'},\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 1.,\n",
    "    'bagging_fraction': 1.,\n",
    "    'bagging_freq': 1,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "print('Starting training...')\n",
    "# train\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=10,\n",
    "                valid_sets=lgb_eval,\n",
    "                early_stopping_rounds=5)\n",
    "\n",
    "print('Saving model...')\n",
    "# save model to file\n",
    "gbm.save_model('model.txt')\n",
    "\n",
    "print('Starting predicting...')\n",
    "# predict\n",
    "y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n",
    "# eval\n",
    "rmse_test = mean_squared_error(y_test, y_pred)\n",
    "print(f'The MSE of prediction is: {rmse_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And also with scikit-learn [HistGradientBoostingRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingRegressor.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE of prediction is: 0.19818811241253062\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.experimental import enable_hist_gradient_boosting  # uncomment this for sklearn < 1.0\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "est = HistGradientBoostingRegressor(\n",
    "    max_iter=10,\n",
    "    validation_fraction=0.2,\n",
    "    random_state=42,\n",
    "    l2_regularization=0.,\n",
    "    min_samples_leaf=20,\n",
    "    learning_rate=0.3,\n",
    "    n_iter_no_change=5).fit(X_train, y_train)\n",
    "y_pred = est.predict(X_test)\n",
    "rmse_test = mean_squared_error(y_test, y_pred)\n",
    "print(f'The MSE of prediction is: {rmse_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's do a round of profiling using snakeviz\n",
    "\n",
    "While lightgbm is implemented in C++ and HistGradientBoostingRegressor with Cython, our TinyGBT is implemented in pure Python. The consequence is that our code is much slower. To identify the computational bottleneck let's profile our code. For this we will use snakeviz.SnakeViz 是一个 用于可视化 Python 程序性能分析（profiling）结果的工具。\n",
    "\n",
    "There are many other profilers such as:\n",
    "- line_profiler: https://github.com/pyutils/line_profiler (easy line by line profiling for Python code)\n",
    "- viztracer: https://viztracer.readthedocs.io (allows to profile low level code including compiled Cython code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting snakeviz\n",
      "  Downloading snakeviz-2.2.2-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: tornado>=2.0 in c:\\users\\lenovo\\anaconda3\\envs\\jupyter\\lib\\site-packages (from snakeviz) (6.5.3)\n",
      "Downloading snakeviz-2.2.2-py3-none-any.whl (183 kB)\n",
      "Installing collected packages: snakeviz\n",
      "Successfully installed snakeviz-2.2.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install snakeviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 5 rounds.\n",
      "Iter   0, Train's L2: 0.3467775744, Valid's L2: 0.3438012841, Elapsed: 8.23 secs\n",
      "Iter   1, Train's L2: 0.1910543084, Valid's L2: 0.2161812292, Elapsed: 8.05 secs\n",
      "Training finished. Elapsed: 16.28 secs\n",
      " \n",
      "*** Profile stats marshalled to file 'C:\\\\Users\\\\lenovo\\\\AppData\\\\Local\\\\Temp\\\\tmpmos3492_'.\n",
      "Embedding SnakeViz in this document...\n",
      "<function display at 0x0000019669E17C40>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<iframe id='snakeviz-63b70c2b-db35-11f0-b498-182649b07ed9' frameborder=0 seamless width='100%' height='1000'></iframe>\n",
       "<script>document.getElementById(\"snakeviz-63b70c2b-db35-11f0-b498-182649b07ed9\").setAttribute(\"src\", \"http://\" + document.location.hostname + \":8080/snakeviz/C%3A%5CUsers%5Clenovo%5CAppData%5CLocal%5CTemp%5Ctmpmos3492_\")</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext snakeviz\n",
    "\n",
    "gbt = GBT(n_estimators=2,  # do only two rounds of boosting\n",
    "          gamma=0., lambd=1,\n",
    "          min_split_gain=0.1, max_depth=5,\n",
    "          learning_rate=0.3)\n",
    "\n",
    "%snakeviz gbt.fit(X_train, y_train, valid_set=(X_test, y_test), early_stopping_rounds=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>EXERCISE</b>:\n",
    "     <ul>\n",
    "         <li>Where is most of the time spent?</li>\n",
    "         <li>What is the complexity of the <code>TreeNode.build</code> method as a function of the number of samples and the number of features?</li>\n",
    "         <li>What could we do about this?</li>\n",
    "     </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
